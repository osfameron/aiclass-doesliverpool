HUH... Video 5.9: Maximum Likelihood 

P(Yi) = { PI   if Yi = S
          1-PI if Yi = H

(Note that PI isn't our PI, it's just a variable)

P(Yi) = PI ^ Yi * (1-PI)^ (1-Yi)

P(data) = PI P(Yi) = PI ^ count(Yi=1) . (1-PI) ^ count(Yi=0)

maximize the logarithm

log P(data) = 3.log PI + 5 log (1-PI)

Ah, this is just a "mathematical derivation" to get the obvious answer.

-----
5.11
P(spam | M="Sports")

    P("sports" | spam) = 1/9
    P("sports" | ham ) = 1/3

p(spam | +"sports") = P(sports | spam) * P(spam)
                      -------------------
                        P(sports)

            = 1/9 * 3/8
              ---------
                P(sports)

and P(sports) = 1/9*3/8  +  1/3*5/8
              = 1/3*1/8  +  5/24 = 6/24 = 1/4
        

        so...  = 1/9 * 3/8            1/24      1/6
                 ---------   =        ---  = 
                    1/4               1/4

========
5.12 P(spam | M="Secret is secret")

 = P( M="Secret is secret" | spam) * P(spam)
    ----------------
    P("Secret is secret")

P(secret | spam) = 3/9
P(secret | ham)  = 1/15
P(is     | spam) = 1/9
P(is     | ham)  = 1/15

P(secret is secret | spam) = 3/9 * 3/9 * 1/9     
  * P(spam), e.g. 3/8 = 1/216
P(secret is secret | ham)  = 1/15 * 1/15 * 1/15
  * P(ham), e.g. 5/8 = 1/5400
P(secret is secret) = 13/2700

         1/216 
        ----
        13/2700

Laplace smoothing

ML  P(x) = Count(X) / N  <-- maximal likelihood

LS(k) P(x) = count(x) + k
             --------
              N + k|x|   e.g. k for every class

      i.e. K is a slight extra amount that we assume for anything

      assume K = 1

#5.14
I'm assuming "spam" and "ham" are the classes?  Yup, looks like!

      P(spam) = 1 + k           1+1
                -----          ---  = 2/3
                1 + k|x|        1+2

                7/12

#5.15 assuming k=1
P(spam) = 3+1 / 8+2 = 4/10 = 2/5
P(ham)  = 5+1 / 10  = 6/10 = 3/5

P(today|SPAM) = 0+1 / 9+12  = 1/21
P(today|ham)  = 2+1 / 15+12 = 3/27

==========

#5.16
M="today is secret"

P(

